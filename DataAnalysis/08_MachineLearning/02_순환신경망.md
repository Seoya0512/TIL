# 순환 신경망 🧠

___

### RNN(Recurrent Neural Network) ➡️ 🔂 ➡️

- 순환 신경망은 각 층의 결괏값이 출력층을 향하면서도 동시에 현재 층의 다음 계산에 사용 
  - 🔛 <u>Feed-Forward Network</u>: 완전연결층, 컨볼루션 신경망은 신경망이 가지고 있는 모든 출력값이 마지막층인 출력 

- RNN은 반복할 때 이전에 계산한 정보를 재사용하는 for  루프

- RNN은 스텝함수에 의해 특화됨 

  > output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)

- **RNN셀**: 순환 신경망은 노드가 출력값을 반환하는 동시에 이전상태(state)를 기억하는 메모리 역할을 수행
- RNN셀은 은닉 상태 

```pseudocode
# 순환 신경망을 표현한 의사코드 

#  첫번째 타임스텝에서 이전 출력이 정의되지 않았음으로 현재 상태가 없음 = 0벡터로 초기화 
state_t = 0 

# 각 시점에 해당하는 입력을 반복 
for input_t in input_sequence:
	# 입력과 은닉 상태를 활성화 함수에 통과
	# 현재 상태(input_features), 크기의 입력 (state_t)
	output_t = activation_func(input_t, state_t)
	# 출력값은 다음 시점을 위한 은닉 상태 
	state_t = output_t
```



 #### RNN예시 - 햄버거 가게 🍔

- 신메뉴 실시를 위한 고객설문 조사는 2회 실시 

> 🍔 간장 소스 ----> 고객 만족도 : 3.0 
>
> 🍔 마요네즈 ---- (고객 설문조사 결과 반영)----> 고객 만족도 : 6.0 
>
> 🍔 비법소스  ----- (고객 설문조사 결과 반영)---> 고객 만족도 : 10.0 

- **현재시점 t **의 햄버거 소스의 선택은 **이전 시점인 t-1시점**의 고객설문조사를 참고 했음을 확인 할 수 있음 



#### RNN의 단점 

- 영화 리뷰와 같이 긴 문장의 데이터(시퀀스)를 처리하기 어려움 

- **Vanishing gradient problem**
  -  앞 부분을 시점이 흐를 수록 지속해서 기억하지 못함, 오래된 값은 영향력이 줄어든다 

- 해결방안 
  - LSTRM 과 GRU 층 사용 

___

### LSTM (Long Short-Term Memory)

✔️ RNN의 그래디언트 소실 문제에 대한 대응방안 

✔️**Cell state** 를 통해 이전 정보를 계속해서 사용하여 그래디언트 손실 문제를 방지하는 것



 - 은닉층인 RNN셀과 Cell state 층이 존재함 

    - 정보를 여러 타임스텝에 걸쳐 나르는 방법이 추가됨 

    - 시퀀스 어느 지점에서 추출된 정보가 '컨베이어 벨트'위로 올라가 필요한 시점의 타임스텝으로 이동되어 떨궈짐

       - 무엇을 저장하고 까먹을지에 대한 판단을 해주고, 그 데이터 를 저장함 

         