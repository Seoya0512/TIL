{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_WordEmbedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbCPXaUmAQEw"
      },
      "source": [
        "# Embedding(2) \n",
        "### Pre-trained word embedding\n",
        "\n",
        "학습시간이 매우 길어서 GPU 사용해야함\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYxtz_EumXB5",
        "outputId": "f5f7d5ca-f317-4bff-c8fd-c17545d27034"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgGqchhzmfHD"
      },
      "source": [
        "imdb_dir = '/content/drive/MyDrive/Data/Encoding/aclImdb'\n",
        "glove = '/content/drive/MyDrive/Data/Encoding/glove.6B.100d.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyyqvYCOPGnY"
      },
      "source": [
        "- imdb raw 데이터를 사용 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft-021d-5xmX"
      },
      "source": [
        "import os\n",
        "\n",
        "train_dir = os.path.join(imdb_dir, \"train\")\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in [\"neg\", \"pos\"]:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname [-4:] == \".txt\":\n",
        "            #f = open(os.path.join(dir_name, fname))\n",
        "            # using code above gave: UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 194: character maps to <undefined>\n",
        "            # added: encoding='utf-8'\n",
        "            f = open(os.path.join(dir_name, fname), encoding='utf-8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == \"neg\":\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vriPHqmR5dvN",
        "outputId": "33d1da78-9b40-460a-8e70-56d870fc9f4e"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing. sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100 # cuts off review after 100 words\n",
        "training_samples = 200 # Trains on 200 samples\n",
        "validation_samples = 10000 # Validates o 10000 samples\n",
        "max_words = 10000 # Considers only the top 10000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index                   # Length: 88582\n",
        "print(\"Found %s unique tokens.\" % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print(\"Shape of data tensor:\", data.shape)\n",
        "print(\"Shape of label tensor:\", labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n",
        "# all negatives first, then all positive\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples] # (200, 100)\n",
        "y_train = labels[:training_samples] # shape (200,)\n",
        "x_val = data[training_samples:training_samples+validation_samples] # shape (10000, 100)\n",
        "y_val = labels[training_samples:training_samples+validation_samples] # shape (10000,)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH_7jsrzPJBT"
      },
      "source": [
        "- 텍스트를 벡터로 만들고 훈련세트와 검증세트로 나눔 \n",
        "- 훈련 데이터를 처음 200개 샘플로 제한 ➡️ 200개 샘플을 학습한 후 영화 리뷰를 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOE0WqJCPa1N"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100\n",
        "training_samples =200\n",
        "validation_samples = 10000\n",
        "max_words = 10000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHhEtyTnfrL3",
        "outputId": "d3b3acd6-de30-46c1-e1db-761e6e4467d2"
      },
      "source": [
        "# 토큰, 벡터화 \n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88582 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B5LvI8ngEDF",
        "outputId": "1bc0a302-b14f-4486-faec-2bcc57977a38"
      },
      "source": [
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# 훈련, 검증 세트로 나누기 \n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tr35w1_gvv4"
      },
      "source": [
        "GloVe Word Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0PVaUwVhLQT"
      },
      "source": [
        "- 파일을 파싱하여 단어와 이에 상응하는 벡터 표현을 매핑하는 인덱스 생성 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCrw1uIKhIWc",
        "outputId": "91562b40-676b-4a74-d69e-6a4cfa88359c"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(glove)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' %len(embeddings_index))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hCR8i9-hyqF"
      },
      "source": [
        "- Embedding 행렬 생성 \n",
        "  - 크기 : (max_words, embedding_dim)\n",
        "  - i번째 원소는 단어 인덱스의 i번째 단어에 상응하는 embedding_dim 차원 벡터 \n",
        "  - 인덱스 0은 어던 단어나 토근도 아닐 경우를 나타냄 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWj6q0L-hw8s"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Ob7Y42iVqd"
      },
      "source": [
        "- 모델 생성 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuF8zomNiTKY",
        "outputId": "8764dbb4-7c32-419d-c17f-523bfde2d30b"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                320032    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "=================================================================\n",
            "Total params: 1,321,088\n",
            "Trainable params: 1,321,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvCZwXNUi4rM"
      },
      "source": [
        "- Embedding 층은 하나의 가중치 행렬을 가짐 \n",
        "  - 2D 부동 소수 행렬, 각 i번째 원소는 i번째 인덱스에 상응하는 벡터\n",
        "  - Embedding 층에 GloVe 행렬을 로드\n",
        "  - 추가적으로 Embedding 층을 동결 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDn6zXsdis9T"
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT6TFvmJjPkW"
      },
      "source": [
        "- 훈련 및 검증 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4PWgoVbzbiV"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10, \n",
        "                    batch_size=32)\n",
        "\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrI2SiBMjmVw"
      },
      "source": [
        "- 그래프로 결과 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xGlbgUBjlHd"
      },
      "source": [
        "# 정확성 확인 \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "# accuracy 그래프 \n",
        "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "# loss 그래프 \n",
        "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}